\section[Des CSU pour les CIOE de Stupfel et Poget 2011]{Des conditions suffisantes pour les CIOE de \cite{stupfel_sufficient_2011}}

  Nous rappelons que nous avons introduit dans le chapitre \ref{sec:context_math} l'opérateur de Calderón liant les champs sur la surface extérieure de l'objet. 

  % \begin{REM}
  %   Tu dois mettre ici un paragraphe sur la notion d'opérateur de Calderón, pour dire qu'une CIOE est une CL sur \(\partial\Omega\) qui remplace le compactant de \(\Omega\).
  % \end{REM}
  % \begin{REP}
  %   Je ne connais pas les opérateurs de Calderón, ni la notion de compactant.
  % \end{REP}

  Nous définissons une \glspl{acr-cioe} liant \(\vE_t\) et \(\vn\pvect\vH\) sur \(\Gamma\) comme une approximation de cet opérateur de Calderón.
  %L'existence des CIOE est en dehors du cadre de cette thèse, nous ne ferons donc qu'utiliser des CIOE existantes.

  Grâce à ces CIOE, nous allons établir des conditions suffisantes qui impliquent la \gls{acr-cgu} \eqref{eq:unicite:form_var:cgu}.
  Comme ces conditions ne sont que suffisantes, il n'y pas une unique \gls{acr-csu} pour une CIOE donnée.
  La difficulté est d'être capable de juger si une CIOE est satisfaisante, et si ce n'est pas le cas, d'être capable d'en proposer une autre.

  Par abus de langage, une CSU peut contenir une ou plusieurs expressions, il est important que ces expressions permettent de déduire la propriété \eqref{eq:unicite:form_var:cgu}.

  Les CIOE de \cite{stupfel_sufficient_2011} font intervenir l'opérateur de Hodge \(\LL\) qui est l'opérateur laplacien vectoriel surfacique, commençons par rappeler son expression et quelques propriétés.

  Définissons les opérateurs différentiels surfaciques (voir annexe \ref{sec:annexe:div_grad_rot}), définis sur \(\Gamma\):
  \begin{align*}
      \vgrads{f}(\vx) &= \vgrad{f}(\vx) - \vn(\vx) (\vn(\vx) \cdot \vgrad{f}(\vx)),
      \\
      \vdivs{\vu} &= \vdiv\left(\vu(\vx) - \vn(\vx) (\vn(\vx) \cdot \vu(\vx)\right),
      \\
      \vrots{\vu}(\vx) &= \vn(\vx)\left(\vn(\vx) \cdot \vrot{\vu}(\vx)\right).
  \end{align*}


  Enfin par abus de notations, nous omettrons tous les dépendances en \(\vx\) dans les intégrales \(\int_\Gamma f(\vx)\vE(\vx)\dd{\Gamma(\vx)}\equiv \int_\Gamma f\vE\).
  \begin{defn}
    \label{def:operator:L}
    % Pour tout \(\vu \in (\mathcal{C}^\infty(\Gamma))^2\)
    \begin{equation*}
        \fonction{\LL}{(\mathcal{C}^\infty(\Gamma))^3}{(\mathcal{C}^\infty(\Gamma))^3}
          {\vu}{\vgrads{\vdivs \vu} - \vrots{\vrots \vu}.}
    \end{equation*}
  \end{defn}

  \begin{prop}
    \label{eq:hodge:negatif}
    L’opérateur hermitien \(\LL\) est symétrique négatif.
  \end{prop}

  \begin{proof}
    Pour tous \(\vu,\vv \in (\mathcal C^\infty(\Gamma))^3\), où \(\Gamma\) n'a pas de bord,
    \begin{align*}
      \int_\Gamma \vu\cdot \LL(\conj{\vv}) &= \int_\Gamma \conj{\vv}\cdot \LL(\vu),
      \\
      \int_\Gamma \vu\cdot \LL(\conj{\vu}) &= -\norm{\vdivs{\vu}}^2 - \norm{\vrots\vu}^2 \le 0.
    \end{align*}
  \end{proof}

  % \begin{REM}
  %   Tu as besoin ici de \(\Gamma\) infiniment régulière ( à vérifier ) ?
  % \end{REM}
  % \begin{REP}
  %   On a toujours considéré la surface infiniment régulière pour calculer les coeffs.
  % \end{REP}

  \begin{prop}
    \label{prop:unicite:injectif:opérateur:L}
    Soit \(\vu \in V=(\mathcal{C}^\infty(\Gamma))^3, a_0 \in \CC^*, a_1 \in \CC\).
    
    Soit \(\mathcal{P}\) l'opérateur de \(V\) vers lui-même tel que \(\mathcal{P}\vu = (a_0\oI + a_1 \LL)\vu\).

    Si \(\Re(a_0)\ge 0\) et \(\Re(a_1)\le 0\) alors l'opérateur \(\mathcal{P}\) est injectif sur \(V\).
  \end{prop}
  \begin{proof}
    Soit \(\vu \in \Ker{\mathcal{P}}\). Donc \(\mathcal{P}\vu  = 0\) ce qui implique
    \begin{align*}
      \int_\Gamma \mathcal{P}\vu\cdot\conj{\vu}  &= 0,
      \\
      & = \int_\Gamma (a_0\oI + a_1 \LL)\vu\cdot\conj{\vu},
      \\
      & = a_0 \norm{{\vu}}^2 - a_1 \left(\norm{\vdivs{\vu}}^2 + \norm{\vrots{\vu}}^2\right).
      \intertext{Donc}
      \Re\left(\int_\Gamma \mathcal{P}\vu\cdot\conj{\vu}\right) &= 0,
      \\
      &=\Re(a_0) \norm{{\vu}}^2 - \Re(a_1) \left(\norm{\vdivs{\vu}}^2 + \norm{\vrots{\vu}}^2\right).
    \end{align*}
    Or \(\Re(a_0)\ge 0\) et \(\Re(a_1)\le 0\), donc tous les termes de l'expression précédente sont nuls, donc \(\norm{\vu}=0\), donc \(\vu=0\).
    Donc l'opérateur \(\mathcal{P}\) est injectif.
  \end{proof}
  
  \begin{prop}
    \label{prop:unicite:inversible:opérateur:L}
    Soit \(\vu \in V=(\mathcal{C}^\infty(\Gamma))^3\).
    
    Soit \(\mathcal{P}\) l'opérateur tel que \(\mathcal{P}\vu = (a_0\oI + a_1 \LL)\vu\).

    Si \(\Re(a_0)\ge 0\) et \(\Re(a_1)\le 0\) alors l'opérateur \(\mathcal{P}\) est bijectif de \(V\).
  \end{prop}
  \begin{proof}
    D'après l'alternative de Fredholm (voir \cite[Théorème~VI.6, p.~92]{brezis_analyse_1996})
    %, si l'opérateur \(\LL\) est compact,%
    alors \(\mathcal{P}\) est soit non-injectif soit bijectif. D'après la proposition \ref{prop:unicite:injectif:opérateur:L}, comme \(\Re(a_0)\ge 0\) et \(\Re(a_1)\le 0\), alors il est injectif, donc bijectif.
    % Montrons que \(\LL\) est compact.
  \end{proof}


  On se donne une condition aux limites sur \(\Gamma\) qui dépend d'un nombre fini de coefficients complexes: \(\vE_t(\vx) = \mathcal{Z}((a_i)_i))(\vn \pvect \vH) (\vx)\).
  On veut trouver des conditions sur ces coefficients \((a_i)_i\) permettant de garantir \eqref{eq:unicite:form_var:cgu}, et donc assurant que \((\vE,\vH) = (0,0)\) est l'unique solution de

  % problème sans source. 

  % On rappelle que l'on veut trouver des conditions permettant de garantir \eqref{eq:unicite:form_var:cgu}, c'est-à-dire \(\Re(X)\ge0\) où \(X = \int_\Gamma \vJ \cdot \conj{\vE_t}\) sachant que \gls{phy-J} est la trace tangentielle sur \(\Gamma\) de l’excitation magnétique (\(\vJ = \vn \pvect \vH\)).

  % \begin{REM}
  % Il faut mieux l'exprimer. " ( qui dépend d'un nombre fini de coefficients généralisant \(a_0,a_1\) ). On veut trouver des conditions sur ces coefficients permettant de garantir et donc assurant que \((\vE,\vH) = (0,0)\) est ..."
  % \end{REM}
  % Ces conditions garantissent que \((\vE,\vH) = (0,0)\) est l'unique solution de 
  \begin{equation}
    \label{eq:unicite:probleme_sans_ci}
    \begin{aligned}
      \left\lbrace
      \begin{aligned}
        \vrot{\vE} + ik_0\vH &= 0
        \\
        \vrot{\vH} - ik_0\vE &= 0
      \end{aligned}
      \right. && \text{dans \(\OO^c_R\)},
      \\
      \Tr(\vE_t) = - \vn_{S_R} \pvect \vH && \text{sur \(S_R\),}
      \\
      \vE_t(\vx) = \mathcal{Z}((a_i)_i))(\vn_\Gamma \pvect \vH) (\vx)  && \text{sur \(\Gamma\).}
    \end{aligned}
  \end{equation}

\begin{remark}
  Nous rappelons que nous notons sur \(\Gamma\), \(\vJ = \vn_\Gamma \pvect \vH\).
\end{remark}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{CSU de la CI0}
    Considérons la condition d’impédance de Leontovich, la \hyperlink{ci0}{CI0} caractérisée par
    \begin{align}
      \label{eq:unicite:ci0}
      \vE_t = a_0 \vJ &&  a_0 \in \CC.
    \end{align}

    \begin{defn}
      \label{def:csu:ci0}
      On définit le sous-espace fermé de \(\CC\)
      \begin{equation*}
        \CSU{CI0} = \lbrace a_0 \in \CC, \Re(a_0) \ge 0 \rbrace.
      \end{equation*}
    \end{defn}

    \begin{prop}[Une CSU pour la CI0]
      \label{prop:csu:ci0}
      Si
      \begin{equation*}
        a_0 \in \CSU{CI0},
      \end{equation*}
      alors le problème \{\eqref{eq:unicite:probleme_sans_ci},\eqref{eq:unicite:ci0}\} a une unique solution.
      % \begin{REM}
      %   alors le problème \{\eqref{eq:unicite:probleme_sans_ci},\eqref{eq:unicite:ci0}\} a une unique solution.
      % \end{REM}
      % \begin{REP}
      % N'est-ce pas la même chose : je nomme la CIOE plutôt que d'y référer par numéro.
      % \end{REP}
      % \begin{REP}
      % Fait
      % \end{REP}
    \end{prop}
    \begin{proof}
      Cela découle de \( X = \conj{a_0}\norm{\vJ}^2\) donc \(\Re(X) = \Re(a_0)\norm{\vJ}^2 \).
    \end{proof}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{CSU de la CI01}
    Considérons la condition d’impédance \hyperlink{ci01}{CI01}:
    \begin{align}
      \label{eq:unicite:ci01}
      \vE_t = (a_0\oI + a_1 \LL)\vJ && \forall (a_0, a_1) \in \CC^2.
    \end{align}

    \begin{defn}
      \label{def:csu:ci01}
      On définit le sous-espace fermé de \(\CC^2\)
      \begin{equation*}
        \CSU{CI01} = \left\lbrace (a_0,a_1) \in \CC^2,
        \begin{matrix}
        \Re\left(a_0\right) \ge 0
        \\
        \Re\left(a_1\right) \le 0
        \end{matrix}
        \right\rbrace.
      \end{equation*}
    \end{defn}

    \begin{prop}[Une CSU pour la CI01]
      \label{prop:csu:ci01}
      Si
      \begin{equation*}
        (a_0,a_1) \in \CSU{CI01},
      \end{equation*}
      alors le problème \{\eqref{eq:unicite:probleme_sans_ci},\eqref{eq:unicite:ci01}\} a une unique solution.
      % \begin{REM}
      %   Je te propose de mettre soit le pb \eqref{eq:unicite:probleme_sans_ci}, CI01  soit  \eqref{eq:unicite:probleme_sans_ci},\eqref{eq:unicite:ci01} (le problème est le système complet).
      % \end{REM}
      % \begin{REP}
      %   Sauf que justement \eqref{eq:unicite:probleme_sans_ci} est "complet", je ne spécifie pas la condition choisie dedans, mais il y en une.
      % \end{REP}
      % \begin{REP}
      % Fait
      % \end{REP}
    \end{prop}
    \begin{proof}
      On a
      \begin{align*}
        X & = \conj{a_0} \norm{\vJ} ^2 - \conj{a_1} \left(\norm{\vdivs{\vJ}}^2 + \norm{\vrots{\vJ}}^2\right), 
        \intertext{donc}
        \Re(X) & = \Re{(a_0)} \norm{\vJ} ^2 - \Re{(a_1)}\left(\norm{\vdivs{\vJ}}^2 + \norm{\vrots{\vJ}}^2\right).
      \end{align*}
      Si l’on suppose \((a_0,a_1) \in \CSU{CI01}\), tous les termes sont positifs, donc \(\Re(X)\ge 0\).
    \end{proof}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{CSU de la CI1}

    Considérons la condition d’impédance \hyperlink{ci1}{CI1}:
    \begin{align}
    \label{eq:unicite:ci1}
      (\oI + b \LL) \vE_t = (a_0\oI + a_1 \LL) \vJ && \forall (a_0, a_1,b) \in \CC^3.
    \end{align}

    Pour cette CIOE, nous observons qu'il y plusieurs sous-espaces différents, inclus dans \(\CC^3\) qui conduisent à l'unicité de la solution du problème.

    %\subsubsection{CSU de \cite{stupfel_sufficient_2011}}

    On définit 
    \begin{equation}
      \label{eq:unicite:delta}
      \begin{matrix}
        \Delta: & \CC^3 &\rightarrow& \CC
        \\
        & (a_0,a_1,b) & \mapsto & a_1 - a_0\conj{b}.
      \end{matrix}
    \end{equation}
    Par abus de notation, on omet les variables \((a_0,a_1,b)\)
    \begin{equation}
       \Delta(a_0,a_1,b) \equiv \Delta.
    \end{equation}

    On rappelle la CSU énoncée dans \cite{stupfel_sufficient_2011}.

    \begin{defn}
      \label{def:csu:ci1-1}

      On définit le sous-espace de \(\CC^3\)
      \begin{equation*}
        \CSU[1]{CI1} = \left\lbrace 
        (a_0,a_1,b) \in \CC^3,
        \begin{matrix}
        \Re(\Delta) = 0
        \\
        \Im(\Delta) \not = 0
        \\
        \Im(\Delta)\Im(b) \ge 0
        \\
        \Im(\Delta )\Im(a_1\conj{a_0})\ge 0
        \end{matrix}
        \right\rbrace.
      \end{equation*}

      Remarquons que, grâce à l'égalité
      \[
      \Im \Delta \Im (a_1\conj{a_0})+\abs{a_0}^2\Im\Delta \Im b = (\Im \Delta)^2\Re a_0,
      \]
      on en déduit que \(\Re a_0\geq 0\) et on retrouve ainsi une condition classique sur \(a_0\), mais qui n'implique pas \(\CSU[1]{CI1}\).

    \end{defn}

    % \begin{REM}
    %   J'ai fait un petit calcul utilisant \(a_1-a_0\conj{b}=i\Im(\Delta)\) et j'en déduis que \(\Re(a_0)\ge0\) donc c'est une condition qui est contenue dans
    %   les autres.
    % \end{REM}
    % \begin{REP}
    %   Y a t'il équivalence ou implication entre \(\Re(\Delta) = 0\) et \(\Re(a_0)\ge0\) ? Si la première, autant s'en servir.
    % \end{REP}

    \begin{prop}[Une première CSU pour la CI1]
      \label{prop:csu:ci1-1}
      On a 
      \begin{equation*}
        (a_0,a_1,b) \in \CSU[1]{CI1} \Rightarrow \Re(X)\ge 0. 
      \end{equation*}
      C'est-à-dire \((a_0,a_1,b) \in \CSU[1]{CI1}\) entraîne l'unicité de la solution du problème \{\eqref{eq:unicite:probleme_sans_ci},\eqref{eq:unicite:ci1}\}.
    \end{prop}
    % \begin{REM}
    %   (\eqref{eq:unicite:probleme_sans_ci}, CI1) ou (\eqref{eq:unicite:probleme_sans_ci},\eqref{eq:unicite:ci1}).
    % \end{REM}
    % \begin{REP}
    % fait
    % \end{REP}
    \begin{proof}
      On utilise l'identité \(\Delta\oI = (a_1(\oI +\conj{b}\LL) - \conj{b}(a_0\oI + a_1\LL))\):
      \begin{align*}
        \Delta X &= \int_\Gamma \left(a_1(\oI +\conj{b}\LL) \vJ\right)\cdot\conj{\vE_t} - \left(\conj{b}(a_0\oI + a_1 \LL)\vJ\right)\cdot\conj{\vE_t}.
        \intertext{Comme l'opérateur \(\LL\) est symétrique,}
        \Delta X &= \int_\Gamma \left(a_1(\oI +\conj{b}\LL) \conj{\vE_t}\right)\cdot\vJ - \int_\Gamma \left(\conj{b}(a_0\oI + a_1 \LL)\vJ\right)\cdot\conj{\vE_t}.
        \intertext{En utilisant la CI1,}
        \Delta X &= \int_\Gamma \left(a_1(\conj{a_0}+\conj{a_1}\LL) \conj{\vJ}\right)\cdot\vJ - \int_\Gamma \left(\conj{b}(\oI +b \LL)\vE_t\right)\cdot\conj{\vE_t}, \\
        \Delta X &= a_1\conj{a_0} \norm{ \vJ }^2 - |a_1|^2 \left(\norm{\vdivs{\vJ}}^2 + \norm{\vrots{\vJ}}^2\right) - \conj{b} \norm{ \vE_t }^2 + |b|^2\left(\norm{\vdivs{\vE_t}}^2 + \norm{\vrots{\vE_t}}^2\right).
      \end{align*}

      % On pose 
      % \begin{align*}
      %   F &= -\int_\Gamma \vJ \LL \conj{\vJ} \ge 0
      %   \\
      %   G &= -\int_\Gamma \vE_t \LL \conj{\vE_t} \ge 0
      % \end{align*}

      La partie imaginaire de \(\Delta X\) est
      \begin{equation*}
        % \Re(\Delta)\Re(X) - \Im(\Delta)\Im(X) &= \Re(a_1\conj{a_0}) \norm{\vJ}^2 - \Re(\conj{b})\norm{\vE_t}^2 -|a_1|^2 F + |b|^2 G \\
        \Im(\Delta X)= \Im(a_1\conj{a_0}) \norm{\vJ}^2 - \Im(\conj{b})\norm{\vE_t}^2.
      \end{equation*}

      Comme \(\Re(\Delta) = 0, \Im(\Delta X) = \Im(\Delta)\Re(X)\), alors si \((a_0,a_1,b)\in\CSU[1]{CI1}\), \(\Im(\Delta)\not=0\) d'où
      \begin{equation*}
        \Im(\Delta)^2\Re(X) = \Im(\Delta)\Im(a_1\conj{a_0}) \norm{\vJ}^2 + \Im(\Delta)\Im({b})\norm{\vE_t}^2.
      \end{equation*}
      et tous les termes du membre de droite sont positifs donc \(\Re(X)\ge0\).

      Dans le cas \(\Delta=0\), la CIOE s'écrit \((\oI+b\LL)E=a_0(\oI+\conj{b}\LL)\vJ\), et on ne peut pas en déduire aisément une condition sur les coefficients \(a_0\) et \(b\).

    \end{proof}

    On remarque que
    \begin{align}
      \CSU[1]{CI1} &\subset \CSU{CI01}\times\CC,
      \\
      \CSU[1]{CI1}\cap(\CC^2 \times \lbrace0\rbrace) &\subsetneq (\CSU{CI01}\times\lbrace0\rbrace).
    \end{align}
     Autrement dit cette CSU de la CIOE CI1 avec \(b=0\) n'est pas équivalente à la CSU de la CI01. 
    % \begin{REM}
    %   A proscrire dans un texte mathématique: veux-tu dire que ce n'est pas la meilleure ?
    % \end{REM}
    % \begin{REP}
    %   Non, qu'on voudrait retrouver les CSU de la CI01, plus une condition sur b.
    % \end{REP}
    Plus précisément, soit \(S = \lbrace (a_0,a_1) \in \CC^2; \Re(a_1)=0 \rbrace\), on a
    \begin{equation}
      \CSU[1]{CI1}\cap(\CC^2 \times \lbrace0\rbrace) = (\CSU{CI01}\cap S) \times\lbrace0\rbrace. 
    \end{equation}

    % \begin{REM}
    %   Cas \(\Delta=0\) ...
    % \end{REM}
    % \begin{REP}
    %   ??? 
    % \end{REP}

    % \begin{lemme}
    % \label{lem:coercivite:opérateur-l}
    %   Soit \(z\in \CC\), \(\vu,\vv \in \Sobolev[1]{(\Gamma)}\) et \(a\) la forme bilinéaire
    %   \begin{equation*}
    %     a(\vect{u},\vect{v}) = \int_\Gamma(\oI + z\LL)\vect{u}\cdot\conj{\vect{v}}.
    %   \end{equation*}
    %   Si \(z\in \CC\backslash \RR_+^*\) alors \(a\) est coercive sur \(\Sobolev[1]{(\Gamma)}\).
    % \end{lemme}
    % \begin{proof}
    %   On cherche à montrer que \(\exists \delta \in \RR^+, \forall \vu \in \Sobolev[1]{(\Gamma)}, |a(\vu,\vu)|^2\ge \delta\left(\norm{\vu}^2+\norm{\vdivs{\vu}}^2+\norm{\vrots{\vu}}^2\right)^2\).
    %   \begin{align*}
    %     |a(\vect{u},\vect{u})|^2 &= \left(\norm{\vu}^2-\Re(z)\left(\norm{\vdivs{\vu}}^2+\norm{\vrots{\vu}}^2\right)\right)^2 + \left(\Im(z)\left(\norm{\vdivs{\vu}}^2+\norm{\vrots{\vu}}^2\right)\right)^2
    %     \\
    %     &= \begin{bmatrix}
    %       \norm{\vu}^2
    %       &
    %       \norm{\vdivs{\vu}}^2+\norm{\vrots{\vu}}^2
    %     \end{bmatrix}
    %     \begin{bmatrix}
    %       1 & - \Re(z)
    %       \\
    %       -\Re(z) & |z|^2
    %     \end{bmatrix}
    %     \begin{bmatrix}
    %       \norm{\vu}^2
    %       \\
    %       \norm{\vdivs{\vu}}^2+\norm{\vrots{\vu}}^2
    %     \end{bmatrix}
    %     \\
    %   \end{align*}
    %   Cette matrice est positive. Mais elle n'est définie que si \((\Im(z))^2\not=0\). Dans ce cas, on a la coercivité \(\Sobolev[1]{(\Gamma})\).

    %   Si \(\Im(z)=0\) alors
    %   \begin{align*}
    %     a(\vect{u},\vect{u}) &= \norm{\vu}^2-\Re(z)\left(\norm{\vdivs{\vu}}^2+\norm{\vrots{\vu}}^2\right)
    %     \\
    %     &\ge \min(1,-\Re(z))\left(\norm{\vu}^2+\norm{\vdivs{\vu}}^2+\norm{\vrots{\vu}}^2\right).
    %   \end{align*}
    %   Il suffit que \(\Re(z) < 0 \) pour avoir la coercivité
    % \end{proof}

    \begin{defn}
      \label{def:csu:ci1-2}

      On définit le sous-espace de \(\CC^3\)
      \begin{equation*}
        \CSU[2]{CI1} = \left\lbrace
        (a_0,a_1,b) \in \CC^3,
        \begin{matrix}
        \Re(b) \le 0
        \\
        \Re\left(a_0\right) \ge 0
        \\
        \Re\left(b\conj{a_0}+\conj{a_1}\right) \le 0
        \\
        \Re\left(b\conj{a_1}\right) \ge 0
        \end{matrix}
        \right\rbrace.
      \end{equation*}
    \end{defn}

    \begin{prop}[Une deuxième CSU pour la CI1]
      \label{prop:csu:ci1-2}
      On a 
      \begin{equation*}
        (a_0,a_1,b) \in \CSU[2]{CI1} \Rightarrow \Re(X)\ge 0. 
      \end{equation*}
      C'est-à-dire \((a_0,a_1,b) \in \CSU[2]{CI1}\) entraîne l'unicité de la solution du problème \{\eqref{eq:unicite:probleme_sans_ci},\eqref{eq:unicite:ci1}\}.
      % \begin{REM}
      %   (\eqref{eq:unicite:probleme_sans_ci}, CI1) ou (\eqref{eq:unicite:probleme_sans_ci},\eqref{eq:unicite:ci1}).
      % \end{REM}
      % \begin{REP}
      %   Fait
      % \end{REP}
    \end{prop}

    \begin{proof}
      Comme on suppose \(\Re(b)\le 0\), alors l'opérateur \(\oI + b\LL\) est injectif d'après la propriété \ref{prop:unicite:injectif:opérateur:L}. Donc pour tout \(\vJ\) dans l'image de \(\oI + b\LL\), il existe un unique \(\vect{D}\) tel que
      \begin{align*}
        (\oI + b \LL)^{-1}\vJ &= \vect{D}.
      \end{align*}
      Donc 
      \begin{align*}
        (\oI + b \LL)\vE_t &= (a_0\oI + a_1\LL)\vJ,
        \\
        (\oI + b \LL)\vE_t &= (a_0\oI + a_1\LL)(\oI + b \LL)\vect{D},
        \\
        0 &= (\oI + b \LL)(\vE_t -  (a_0\oI + a_1\LL)\vect{D}).
      \end{align*}
      L'opérateur \(\oI + b \LL\) est injectif donc \(\vE_t = (a_0\oI + a_1\LL)\vect{D}\), on en déduit alors \(\int_\Gamma \vJ \cdot \conj{\vE_t} = \int_\Gamma \vJ \cdot (\conj{a_0}\oI + \conj{a_1}\LL)\conj{\vect{D}}\).
      Comme \(\vJ = (\oI + b \LL)\vect{D}, X = \int_\Gamma (\oI + b \LL)\vect{D} \cdot (\conj{a_0}\oI + \conj{a_1}\LL)\conj{\vect{D}}\).
      Ainsi, \( X = \conj{a_0}\norm{\vect{D}}^2 - (b\conj{a_0}+\conj{a_1})\left(\norm{\vdivs{\vect{D}}}^2+\norm{\vrots{\vect{D}}}^2\right) + b\conj{a_1} \norm{\LL\vect{D}}^2\).
    \end{proof}

    On remarque que
    \begin{align}
      \CSU[2]{CI1} & \subset \CSU{CI01}\times\lbrace0\rbrace
      \\
      \CSU[2]{CI1}\cap(\CC^2 \times \lbrace0\rbrace) &= (\CSU{CI01}\times\lbrace0\rbrace).
    \end{align}

    Si \(b=0\) la CIOE CI1 se réduit à la CI01 et la \CSU[2]{CI1} est donc meilleure que la  \CSU[1]{CI1}.
